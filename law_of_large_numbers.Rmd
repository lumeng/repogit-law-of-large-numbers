---
title: "Law of Large Numbers"
author: "Meng Lu <lumeng.dev@gmail.com>"
date: "September 27, 2014"
#output: ioslides_presentation
output: html_document
---

A compiled note showing the difference between Strong Law of Large Numbers and Weak 
Law of Large Numbers.  

References and source of material:

* [stackoverflow.com thread "In R, how to draw multiple curves with ggvis?"](http://stackoverflow.com/questions/26132198/in-r-how-to-draw-multiple-curves-with-ggvis)

* [stackoverflow.com thread "Convergence in probability vs. almost sure convergence"](http://stats.stackexchange.com/questions/2230/convergence-in-probability-vs-almost-sure-convergence)


```{r, echo = FALSE, message = FALSE}
library(knitr)
library(ggvis)
library(ggplot2)
library(shiny)
library(dplyr)
library(magrittr)

# Set up default dimensions. Width and height are multiplied by dpi to get
# pixel dimensions.

#knitr::opts_chunk$set(fig.width = 4, fig.height = 3)
```


## Strong Law of Large Numbers (SLLN)


Let $\{X_i\}$ be a sequence of I.I.D. random variables with Bernoulli 
distribution with mean $\mathbb{E}\left(X_i\right) = 0$:

$$X_i \sim B(p=0.5) - 0.5, \quad i = 1, 2, \cdots, n.$$


The average of each sample is an estimator of $\mathbb{E}\left(X_i\right)$:

$$\hat{X}_k = \frac{S_k}{k} = \frac{1}{k} \sum_{i=1}^k X_i, \quad k=1,2, \cdots, n.$$

According to the SLLN, the estimator $\hat{X}_k$ has almost sure convergence to 
$\mathbb{E}\left(X_i\right) = p - 0.5 = 0$:

$$\mathbb{P}\left[\lim_{k \longrightarrow \infty} \left(\hat{X}_k = 0\right)\right] = 1$$

Plot $\hat{X}_k$ as a function of $k$:

```{r, message = FALSE}
n_max <- 10000 
m <- 10 
e <- 0.05
n = seq.int(1, n_max, 1)
s <- cumsum(2*(rbinom(n_max, size=1, prob=0.5) - 0.5))
x_estimate = s/n
data <- data.frame(n, x_estimate)
data %>%
    ggvis(x = ~n, y = ~x_estimate) %>%
    layer_lines() %>%
    add_axis("x", title="k") %>%
    add_axis("y", title="X_k") %>%
    scale_numeric("y", domain = c(-0.3, 0.3), nice = FALSE, clamp = TRUE)
```

SSLN and almost sure convergences implies that it is certain, i.e. with probability 1, that after some sufficiently large but finite $n$, the difference between the curve and the expected value of 0 is arbitrarily small.  This means scientific measurements
of a random variable can obtain approximation to the expected value by doing multiple
measurements and taking average, and the error will certainly decrease after 
some sufficiently large number of measurements.  However, in reality, that threshold
value of $n$ is not known.

## Weak Law of Large Numbers (WLLN)

According to the WLLN, the estimator $\hat{X}_k$ has convergence in probability to $\mathbb{E}\left(X_i\right)  = 0$:

$$\lim_{k \longrightarrow \infty} \mathbb{P}\left [ \lvert \hat{X}_k - 0 \rvert > \epsilon \right] = 0$$

Plot $\hat{X}_k$ as a function of $k$:

```{r, message = FALSE, eval=FALSE}
x <- matrix(2*(rbinom(n_max*m, size=1, prob=0.5) - 0.5), ncol = m)
y <- apply(x, 2, function(z) cumsum(z)/seq_along(z))
matplot(y, type = "l", ylim = c(-0.5, 0.5))
abline(h = c(-e,e), lty = 2, lwd = 2)
```

Or use `ggvis`:

```{r, message = FALSE}
n <- seq.int(1, n_max, 1)
s <- matrix(2*(rbinom(n_max*m, size=1, prob=0.5) - 0.5), ncol = m)
x_estimate <- apply(s, 2, function(z) cumsum(z)/seq_along(z))

data <- data.frame(x_estimate, n)
v <- data %>% 
    ggvis(x = ~n)
for ( i in 1:m) {
    v <- (v %>% layer_paths(prop("y", as.name(colnames(data)[i]))))
}
v %>%
  scale_numeric("y", domain = c(-0.3, 0.3), nice = FALSE, clamp = TRUE, label = 'y')
```

WSLN and convergence in probability implies that the distributions of the curves are centered around y=0, and for any arbitrarily narrow interval around y=0 and any arbitrarily large probability threashold, there exist an sufficiently large threshold value of n such that when n is larger than the threshold, the probability of the curves being inside the narrow interval is larger than the probability threshold. This is just because sample variance $s^2 = \sigma^2/n$ approaches 0 as n increases. But on the other hand, at any n, no matter how large, there are some curves that are outside of that interval. It is in this sense WSLN is weaker than SLLN.