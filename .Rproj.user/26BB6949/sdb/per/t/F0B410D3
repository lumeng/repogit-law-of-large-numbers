{
    "contents" : "---\ntitle: \"Law of Large Numbers\"\nauthor: \"Meng Lu <lumeng.dev@gmail.com>\"\ndate: \"September 27, 2014\"\n#output: ioslides_presentation\noutput: html_document\n---\n\nA compiled note showing the difference between Strong Law of Large Numbers and Weak \nLaw of Large Numbers.  \n\nReferences and source of material:\n\n* [stackoverflow.com thread \"In R, how to draw multiple curves with ggvis?\"](http://stackoverflow.com/questions/26132198/in-r-how-to-draw-multiple-curves-with-ggvis)\n\n* [stackoverflow.com thread \"Convergence in probability vs. almost sure convergence\"](http://stats.stackexchange.com/questions/2230/convergence-in-probability-vs-almost-sure-convergence)\n\n\n```{r, echo = FALSE, message = FALSE}\nlibrary(knitr)\nlibrary(ggvis)\nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(dplyr)\nlibrary(magrittr)\n\n# Set up default dimensions. Width and height are multiplied by dpi to get\n# pixel dimensions.\n\n#knitr::opts_chunk$set(fig.width = 4, fig.height = 3)\n```\n\n\n## Strong Law of Large Numbers (SLLN)\n\n\nLet $\\{X_i\\}$ be a sequence of I.I.D. random variables with binomial distribution with mean $\\mathbb{E}\\left(X_i\\right) = 0$:\n\n$$X_i \\sim B(N=1, p=0.5) - 0.5, \\quad i = 1, 2, \\cdots, n.$$\n\n\nThe average of each sample is an estimator of $\\mathbb{E}\\left(X_i\\right)$:\n\n$$\\hat{X}_k = \\frac{S_k}{k} = \\frac{1}{k} \\sum_{i=1}^k X_i, \\quad k=1,2, \\cdots, n.$$\n\nAccording to the SLLN, the estimator $\\hat{X}_k$ has almost sure convergence to \n$\\mathbb{E}\\left(X_i\\right) = N p - 0.5 = 1 \\times 0.5 - 0.5 = 0$:\n\n$$\\mathbb{P}\\left[\\lim_{k \\longrightarrow \\infty} \\left(\\hat{X}_k = 0\\right)\\right] = 1$$\n\nPlot $\\hat{X}_k$ as a function of $k$:\n\n```{r, message = FALSE}\nn_max <- 10000 \nm <- 10 \ne <- 0.05\nn = seq.int(1, n_max, 1)\ns <- cumsum(2*(rbinom(n_max, size=1, prob=0.5) - 0.5))\nx_estimate = s/n\ndata <- data.frame(n, x_estimate)\ndata %>%\n    ggvis(x = ~n, y = ~x_estimate) %>%\n    layer_lines() %>%\n    add_axis(\"x\", title=\"k\") %>%\n    add_axis(\"y\", title=\"X_k\") %>%\n    scale_numeric(\"y\", domain = c(-0.3, 0.3), nice = FALSE, clamp = TRUE)\n```\n\nSSLN and almost sure convergences implies that it is certain, i.e. with probability 1, that after some sufficiently large but finite $n$, the difference between the curve and the expected value of 0 is arbitrarily small.  This means scientific measurements\nof a random variable can obtain approximation to the expected value by doing multiple\nmeasurements and taking average, and the error will certainly decrease after \nsome sufficiently large number of measurements.  However, in reality, that threshold\nvalue of $n$ is not known.\n\n## Weak Law of Large Numbers (WLLN)\n\nAccording to the WLLN, the estimator $\\hat{X}_k$ has convergence in probability to $\\mathbb{E}\\left(X_i\\right)  = 0$:\n\n$$\\lim_{k \\longrightarrow \\infty} \\mathbb{P}\\left [ \\lvert \\hat{X}_k - 0 \\rvert > \\epsilon \\right] = 0$$\n\nPlot $\\hat{X}_k$ as a function of $k$:\n\n```{r, message = FALSE, eval=FALSE}\nx <- matrix(2*(rbinom(n_max*m, size=1, prob=0.5) - 0.5), ncol = m)\ny <- apply(x, 2, function(z) cumsum(z)/seq_along(z))\nmatplot(y, type = \"l\", ylim = c(-0.5, 0.5))\nabline(h = c(-e,e), lty = 2, lwd = 2)\n```\n\nOr use `ggvis`:\n\n```{r, message = FALSE}\nn <- seq.int(1, n_max, 1)\ns <- matrix(2*(rbinom(n_max*m, size=1, prob=0.5) - 0.5), ncol = m)\nx_estimate <- apply(s, 2, function(z) cumsum(z)/seq_along(z))\n\ndata <- data.frame(x_estimate, n)\nv <- data %>% \n    ggvis(x = ~n)\nfor ( i in 1:m) {\n    v <- (v %>% layer_paths(prop(\"y\", as.name(colnames(data)[i]))))\n}\nv %>%\n  scale_numeric(\"y\", domain = c(-0.3, 0.3), nice = FALSE, clamp = TRUE, label = 'y')\n```\n\nWSLN and convergence in probability implies that the distributions of the curves are centered around y=0, and for any arbitrarily narrow interval around y=0 and any arbitrarily large probability threashold, there exist an sufficiently large threshold value of n such that when n is larger than the threshold, the probability of the curves being inside the narrow interval is larger than the probability threshold. This is just because sample variance $s^2 = \\sigma^2/n$ approaches 0 as n increases. But on the other hand, at any n, no matter how large, there are some curves that are outside of that interval. It is in this sense WSLN is weaker than SLLN.",
    "created" : 1412184758801.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4166988792",
    "id" : "F0B410D3",
    "lastKnownWriteTime" : 1412184760,
    "path" : "~/Dropbox/WorkSpace-Dropbox/Computing/Programming/R/repogit-law-of-large-numbers/law_of_large_numbers.Rmd",
    "project_path" : "law_of_large_numbers.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}